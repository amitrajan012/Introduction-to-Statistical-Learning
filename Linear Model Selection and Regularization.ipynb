{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.text_cell_render { /* Customize text cells */\n",
       "font-family: 'Times New Roman';\n",
       "font-size:1.3em;\n",
       "line-height:1.4em;\n",
       "padding-left:1.5em;\n",
       "padding-right:1.5em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "div.text_cell_render { /* Customize text cells */\n",
    "font-family: 'Times New Roman';\n",
    "font-size:1.3em;\n",
    "line-height:1.4em;\n",
    "padding-left:1.5em;\n",
    "padding-right:1.5em;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Linear Model Selection and Regularization</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Befor moving to non-linear models, there are certain other fitting procedures through which a plain linear model can be improved. These alternate fitting procedures can yield better <b>prediction accuracy</b> and <b>model interpretability</b>.\n",
    "\n",
    " - <b>Prediction Accuracy:</b> Provided that the relationship between predictors and response is linear, the <b>least square estimates will have low bias</b>. If n >> p, this model will have low variance as well. If n is not much larger than p, the model will have a lot of variability, resulting in overfitting and poor prediction accuracy. If p > n, the variance is almost infinite and hence the plain least square method can not be used at all.\n",
    " \n",
    " \n",
    " - <b>Model Interpretability:</b> There may be the case that some of the predictors used in the linear model are not significant. Including such <b>irrelevant variables</b> may lead to unnecessary complexity in the model. These variables can be excluded from the model by <b>feature selection</b> or <b>variable selection</b> and hence a better model interpretability.\n",
    " \n",
    "Some methods that can be used to improve least square fit are:\n",
    "\n",
    " - <b>Subset Selection:</b> The predictors that are statistically significant are identified and the model using least squares is then fit on these reduced set of variables.\n",
    " \n",
    " \n",
    " - <b>Shrinkage:</b> The model is fit using all $p$ predictors and later on the estimated coefficients are shrunken towards 0 relative to the least square estimates. This shrinkage (also called as <b>regularization</b>) can reduce the variance and hence results in better fit.\n",
    " \n",
    " \n",
    " - <b>Dimension Reduction:</b> This method can be used to project the $p$ predictors into a $M$-dimensional space, where $M < p$. These $M$ predictors are then used to fit a least squares linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Subset Selection\n",
    "\n",
    "#### 6.1.1 Best Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In <b>best subset selection</b>, a seperate least squares regression is fitted for each possible combination of $p$ predictors. We then identify the best model out of these. The algorithm for this is as follows:\n",
    "\n",
    "\n",
    " - Let $M_0$ denotes the <b>null model</b>, which contains no predictors. This model simply predicts the sample means of each observation.\n",
    " \n",
    " - For $K=1, 2, ..., p$, all $\\dbinom{p}{k}$ models are fitted and the best among them is denoted as $M_k$. The bset is the one which has the <b>smallest RSS</b> or <b>largest R$^2$</b>.\n",
    " \n",
    " - Single best model out of $M_0, M_1, ..., M_k$ is selected using <b>cross-validation, AIC, BIC</b> or <b>adjusted R$^2$</b>.\n",
    " \n",
    " \n",
    "After first two steps, the problem is reduced from $2^p$ possible models to $p+1$ possible models. Selecting one of the best models out of these $p+1$ models can be misleading. As number of predictors in the model increases, the <b>RSS monotonically decreases</b> and <b>R$^2$ monotonically increases</b> (as they depend on training error and training error improves as we keep on adding new predictors). Hence, we need to choose a model based on <b>test error</b>. This is the reason <b>cross-validation, $C_p$, BIC</b> or <b>adjusted R$^2$</b> is used in this step.\n",
    "\n",
    "In the best subset selection for logistic regression, insted of RSS or R$^2$, we use the <b>deviance</b>. Deviance is negative two times the maximized log-likelihood. The <b>smaller the deviance, the better the fit</b>. For large number of predictors, best subset selection is <b>computationally inefficient</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.2 Stepwise Selection\n",
    "##### Forward Stepwise Selection\n",
    "\n",
    "<b>Forward Stepwise Selection</b> begins with no predictors and then add them to the model one-at-a-time, until all the predictors are in the model. At each step, the variable that gives the greatest <b>additional</b> improvement to the fit is added to the model. The algorithm is as follows:\n",
    "\n",
    " - Let $M_0$ denotes the null model.\n",
    " - Add a single predictor to the model which gives the <b>smallest RSS</b> or <b>largest R^$2$</b> for it. The model is denoted as $M_1$.\n",
    " - Gradually get the models $M_0, M_1, M_2, ... , M_p$ by following the same procedure.\n",
    " - Select the single best model out of these based on <b>cross-validation, C$_p$, BIC</b> or <b>adjusted R$^2$</b>.\n",
    " \n",
    "\n",
    "For $p$ predictors, the forward stepwise selection requires fitting of $1+\\frac{p(p+1)}{2}$ models. This is a substantial improvemenet over best subset selection. Though forward stepwise selection do well in practice, it does not guarantee the selection of best possible model out of all the possible $2^p$ models as all of them are not analyzed in this case.\n",
    "\n",
    "Forward stepwise selection can be applied in the case when $n<p$ as well. But in this case the constructed sub-models range from $M_0$ to $M_{n-1}$, as least square method does not give a unique solution if $p \\geq n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Backward Stepwise Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Backward Stepwise Selection</b> begins with the model containing all the $p$ predictors and it iteratively removes the least useful predictor one at a time. The algorithms is similar to the ones descibed above. Like forward stepwise selection, we need to fit only $1+\\frac{p(p+1)}{2}$ models in backward stepwise selection. It does not guarantee to yield the best model as well. As in this case, we start from fitting the model with all the $p$ predictors, it is needed that: $n > p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hybrid Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In <b>Hybrid Approaches</b>, forward and backward stepwise selection are used to mimic best subset selection. In this method while doing forward stepwise selection, the least significant predictor (the variable that no longer provide any improvement in the model fit) is removed as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.3 Choosing the Optimal Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last step of the algorithms described above, we need to choose the best model out of possible $(p+1)$ models. If we do this selection based on the parameters which are derived on the basis of training error (RSS or R$^2$), we will always end up selecting the model which has all the parameters. For the selection of the best model on the basis of <b>test error</b>, test error can be estimated as:\n",
    "\n",
    " - By doing <b>adjustment</b> to the training error.\n",
    " - can be directly estimated using a cross-validation approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $C_p$, AIC, BIC, and Adjusted R$^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, <b>training MSE</b> (MSE = $\\frac{RSS}{n}$) is an underestimate of test MSE and hence the training RSS and R$^2$ can not be used to select the best model out of all possible ones. There are several techniques that can be used for <b>adjusting</b> the training error to get an estimate of test error. Some of these are : <b>$C_p$, Akaike information criterion (AIC), Bayesian information criterion (BIC)</b> and <b>Adjusted R$^2$</b>.\n",
    "\n",
    "$C_p$ estimate of the test MSE is given as:\n",
    "\n",
    "$$C_p = \\frac{1}{n}(RSS + 2d\\widehat{\\sigma}^2)$$\n",
    "\n",
    "where $d$ is the number of predictors and $\\widehat{\\sigma}^2$ is the estimate of the variance of error $\\epsilon$ associated with each response in the linear model given as:\n",
    "\n",
    "$$Y = \\beta_0 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p + \\epsilon$$\n",
    "\n",
    "The penality added is $2d\\widehat{\\sigma}^2$, which increases as the number of predictors increases. <b>If $\\widehat{\\sigma}^2$ is an unbiased estimator of $\\sigma^2$, then $C_p$ is an unbiased estimate of test MSE.</b> Finally,  model with the lowest value of $C_p$ is chosen.\n",
    "\n",
    "The <b>AIC</b> criterion is used for the models fit by maximum likelihood. AIC is given as:\n",
    "\n",
    "$$C_p = \\frac{1}{n\\widehat{\\sigma}^2}(RSS + 2d\\widehat{\\sigma}^2)$$\n",
    "\n",
    "where the terms has the same meaning as explained above. $C_p$ and AIC are proportional to each other.\n",
    "\n",
    "<b>BIC</b> is derived from a Bayesian point of view. For a least square model, BIC is given as:\n",
    "\n",
    "$$BIC = \\frac{1}{n}(RSS + log(n)d\\widehat{\\sigma}^2)$$\n",
    "\n",
    "As instead of 2, BIC has a factor of $log(n)$, for values of $n>7$, BIC ends up penalizing models with higher predictors more and hence results in a selection of smaller models than $C_p$.\n",
    "\n",
    "The <b>Adjusted R$^2$</b> statistic is another measure for model selection. $R^2$ is defined as $1 - \\frac{RSS}{TSS}$, where RSS is <b>Residual sum of squares</b> and TSS is <b>Total sum of squares</b>. TSS is given as $\\sum(y_i - \\bar{y})^2$. For a least squares model with $d$ predictors, Adjusted R$^2$ statistic is given as:\n",
    "\n",
    "$$Adjusted \\ R^2 = 1 - \\frac{RSS/(n-d-1)}{TSS/(n-1)}$$\n",
    "\n",
    "A <b>large value of adjusted R$^2$ indicates smaller test error</b>. The concept behind adjusted R$^2$ is the fact that once the model is saturated (in terms of predictor), adding more predictors will only decrease RSS by a lower value but as the denominator $(n-d-1)$ decreases by a larger proportion, the value $\\frac{RSS}{n-d-1}$ will overall increase, decreasing the value of adjusted R$^2$.\n",
    "\n",
    "Here the expression for AIC, BIC and $C_p$ is given for linear model fit with least squares. The general expression for them can be computed as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation and Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of computing the above parameters, we can directly estimate the test error by cross-validation methods. This process has an advantage comared to the methods discussed above as it provides a direct estimate of the test error and makes fewer assumption about the underlying model. It can also be used in the cases when it is hard to identify the number of predictors in the model or estimation of error variance $\\sigma^2$ is difficult.\n",
    "\n",
    "<b>As the estimate of test MSE depends on the validation set or the method of cross-validation(the estimate will be different fro 5-fold and 10-fold cross-validation), for the models with slight variance in the estimated test MSE across the variation in the number of predictors, there is a chance that if the validation process is repeated with different split of data, the model selection will change.</b> For this reason, we follow <b>one-standard-error rule</b>. The rule suggests that the lowest point in the curve for the estimated test MSE is found, and eventually, the smallest model (model with smallest number of predictors) for which the estimated test error is within one standard error away from the lowest point is selected as the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Shrinkage Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative to subset selection methods, a model containing all the <b>$p$ predictors</b> can be fit using a technique that <b>constrains</b> or <b>regularizes</b> the coefficient estimates (or <b>shrinks</b> the coefficeint estimates towards 0). Two best known techniques for shrinking the coefficient estimates towards 0 are: <b>ridge regression</b> and the <b>lasso</b>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.1 Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a least squares fitting, the parameters of the model is estimated by minimizing\n",
    "\n",
    "$$RSS = \\sum_{i=1}^{n} \\bigg( y_i - (\\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij})\\bigg)^2$$\n",
    "\n",
    "In <b>ridge regression</b>, coefficients are estimated by minimizing the following term instead:\n",
    "\n",
    "$$RSS + \\lambda \\sum_{j=1}^{p} \\beta_j^2 = \\sum_{i=1}^{n} \\bigg( y_i - (\\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij})\\bigg)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2$$\n",
    "\n",
    "where $\\lambda \\geq 0$ is called as the <b>tuning parameter</b> and is determined separately. In ridge regression, the model fits the data well by minimizing RSS and <b>shrinking penalty</b> ($\\lambda \\sum_{j=1}^{p} \\beta_j^2$). Shrinking penalty will be small when $\\beta_i$s will be close to 0 and hence minimizing this has the effect of shrinking the coefficient estimates towards 0. When the <b>tuning parameter</b> $\\lambda$ is 0, ridge regression will give the least squares estimates. For the larger value of $\\lambda$, the impact of shrinkage penalty increases and hence the coefficient estimates approach closer to 0. We can generate different sets of coefficient estimates for different values of $\\lambda$. The best estimate can be chosen using several cross-validation methods. It is to be noted that the <b>shrinkage penalty is not applied to the intercept</b>.\n",
    "\n",
    "When value of $\\lambda$ is very large, all the ridge regression coefficients approach 0, giving us the <b>null model</b> (model whic contain only intercept and no predictors). In aggregate, the ridge regression coefficients tend to decrease as $\\lambda$ increases but some of the individual coefficients may increase. This phenomenon is shown in the left hand side figure below.\n",
    "\n",
    "<img src=\"images/ridge.PNG\"  width=\"1000px\">\n",
    "\n",
    "Instead we can plot the ridge coefficients against the ratio of <b>l$_2$ norms</b> of the ridge coefficients vector and least squares regression coefficients vector (intercept is excluded). Its value ranges from 1 (when $\\lambda = 0$, the ridge coefficients and least squares coefficients are same) to 0 (when $\\lambda = \\infty$, ridge coefficients aprroach 0). This plot is shown in the right hand side figure above.\n",
    "\n",
    "The standard least squares coefficients are <b>scale invariant</b>. This means that multiplying a predictor by a constant $c$, has an effect of scaling down the least squares coefficients by $\\frac{1}{c}$. In contrast, when multiplying a predictor by $c$, the ridge coefficient changes substantially. Hence, <b>it is best to apply the ridge regression after standardizing the predictors</b> as:\n",
    "\n",
    "$$\\tilde{x_{ij}} = \\frac{x_{ij}}{\\sigma_j} = \\frac{x_{ij}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (x_{ij} - \\bar{x_j})^2}}$$\n",
    "\n",
    "Here the denominator is the <b>estimated standard deviation of the $j$th predictor</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why Does Ridge Regression Improve Over Least Squares?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ridge regression, as $\\lambda$ increases, the fliexibility of the model decreases, leading to low variance and hence increased bias. The model with least squares coefficient estimates(corresponding to $\\lambda = 0$), has high variance but there is less bias. But as $\\lambda$ increases, the shrinkage of the model coefficients leads to a substantial decrease in the model variance with a slight increase in the bias. As $\\lambda$ increases further, the decrease in variance is surpassed by the increase in bias. Hence, <b>minimum MSE is achieved at an intermediate value of $\\lambda$</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
