{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.text_cell_render { /* Customize text cells */\n",
       "font-family: 'Times New Roman';\n",
       "font-size:1.3em;\n",
       "line-height:1.4em;\n",
       "padding-left:1.5em;\n",
       "padding-right:1.5em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "div.text_cell_render { /* Customize text cells */\n",
    "font-family: 'Times New Roman';\n",
    "font-size:1.3em;\n",
    "line-height:1.4em;\n",
    "padding-left:1.5em;\n",
    "padding-right:1.5em;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Linear Model Selection and Regularization</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Befor moving to non-linear models, there are certain other fitting procedures through which a plain linear model can be improved. These alternate fitting procedures can yield better <b>prediction accuracy</b> and <b>model interpretability</b>.\n",
    "\n",
    " - <b>Prediction Accuracy:</b> Provided that the relationship between predictors and response is linear, the <b>least square estimates will have low bias</b>. If n >> p, this model will have low variance as well. If n is not much larger than p, the model will have a lot of variability, resulting in overfitting and poor prediction accuracy. If p > n, the variance is almost infinite and hence the plain least square method can not be used at all.\n",
    " \n",
    " \n",
    " - <b>Model Interpretability:</b> There may be the case that some of the predictors used in the linear model are not significant. Including such <b>irrelevant variables</b> may lead to unnecessary complexity in the model. These variables can be excluded from the model by <b>feature selection</b> or <b>variable selection</b> and hence a better model interpretability.\n",
    " \n",
    "Some methods that can be used to improve least square fit are:\n",
    "\n",
    " - <b>Subset Selection:</b> The predictors that are statistically significant are identified and the model using least squares is then fit on these reduced set of variables.\n",
    " \n",
    " \n",
    " - <b>Shrinkage:</b> The model is fit using all $p$ predictors and later on the estimated coefficients are shrunken towards 0 relative to the least square estimates. This shrinkage (also called as <b>regularization</b>) can reduce the variance and hence results in better fit.\n",
    " \n",
    " \n",
    " - <b>Dimension Reduction:</b> This method can be used to project the $p$ predictors into a $M$-dimensional space, where $M < p$. These $M$ predictors are then used to fit a least squares linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Subset Selection\n",
    "\n",
    "#### 6.1.1 Best Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In <b>best subset selection</b>, a seperate least squares regression is fitted for each possible combination of $p$ predictors. We then identify the best model out of these. The algorithm for this is as follows:\n",
    "\n",
    "\n",
    " - Let $M_0$ denotes the <b>null model</b>, which contains no predictors. This model simply predicts the sample means of each observation.\n",
    " \n",
    " - For $K=1, 2, ..., p$, all $\\dbinom{p}{k}$ models are fitted and the best among them is denoted as $M_k$. The bset is the one which has the <b>smallest RSS</b> or <b>largest R$^2$</b>.\n",
    " \n",
    " - Single best model out of $M_0, M_1, ..., M_k$ is selected using <b>cross-validation, AIC, BIC</b> or <b>adjusted R$^2$</b>.\n",
    " \n",
    " \n",
    "After first two steps, the problem is reduced from $2^p$ possible models to $p+1$ possible models. Selecting one of the best models out of these $p+1$ models can be misleading. As number of predictors in the model increases, the <b>RSS monotonically decreases</b> and <b>R$^2$ monotonically increases</b> (as they depend on training error and training error improves as we keep on adding new predictors). Hence, we need to choose a model based on <b>test error</b>. This is the reason <b>cross-validation, $C_p$, BIC</b> or <b>adjusted R$^2$</b> is used in this step.\n",
    "\n",
    "In the best subset selection for logistic regression, insted of RSS or R$^2$, we use the <b>deviance</b>. Deviance is negative two times the maximized log-likelihood. The <b>smaller the deviance, the better the fit</b>. For large number of predictors, best subset selection is <b>computationally inefficient</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.2 Stepwise Selection\n",
    "##### Forward Stepwise Selection\n",
    "\n",
    "<b>Forward Stepwise Selection</b> begins with no predictors and then add them to the model one-at-a-time, until all the predictors are in the model. At each step, the variable that gives the greatest <b>additional</b> improvement to the fit is added to the model. The algorithm is as follows:\n",
    "\n",
    " - Let $M_0$ denotes the null model.\n",
    " - Add a single predictor to the model which gives the <b>smallest RSS</b> or <b>largest R^$2$</b> for it. The model is denoted as $M_1$.\n",
    " - Gradually get the models $M_0, M_1, M_2, ... , M_p$ by following the same procedure.\n",
    " - Select the single best model out of these based on <b>cross-validation, C$_p$, BIC</b> or <b>adjusted R$^2$</b>.\n",
    " \n",
    "\n",
    "For $p$ predictors, the forward stepwise selection requires fitting of $1+\\frac{p(p+1)}{2}$ models. This is a substantial improvemenet over best subset selection. Though forward stepwise selection do well in practice, it does not guarantee the selection of best possible model out of all the possible $2^p$ models as all of them are not analyzed in this case.\n",
    "\n",
    "Forward stepwise selection can be applied in the case when $n<p$ as well. But in this case the constructed sub-models range from $M_0$ to $M_{n-1}$, as least square method does not give a unique solution if $p \\geq n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Backward Stepwise Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Backward Stepwise Selection</b> begins with the model containing all the $p$ predictors and it iteratively removes the least useful predictor one at a time. The algorithms is similar to the ones descibed above. Like forward stepwise selection, we need to fit only $1+\\frac{p(p+1)}{2}$ models in backward stepwise selection. It does not guarantee to yield the best model as well. As in this case, we start from fitting the model with all the $p$ predictors, it is needed that: $n > p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hybrid Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In <b>Hybrid Approaches</b>, forward and backward stepwise selection are used to mimic best subset selection. In this method while doing forward stepwise selection, the least significant predictor (the variable that no longer provide any improvement in the model fit) is removed as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.3 Choosing the Optimal Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last step of the algorithms described above, we need to choose the best model out of possible $(p+1)$ models. If we do this selection based on the parameters which are derived on the basis of training error (RSS or R$^2$), we will always end up selecting the model which has all the parameters. For the selection of the best model on the basis of <b>test error</b>, test error can be estimated as:\n",
    "\n",
    " - By doing <b>adjustment</b> to the training error.\n",
    " - can be directly estimated using a cross-validation approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $C_p$, AIC, BIC, and Adjusted R$^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, <b>training MSE</b> (MSE = $\\frac{RSS}{n}$) is an underestimate of test MSE and hence the training RSS and R$^2$ can not be used to select the best model out of all possible ones. There are several techniques that can be used for <b>adjusting</b> the training error to get an estimate of test error. Some of these are : <b>$C_p$, Akaike information criterion (AIC), Bayesian information criterion (BIC)</b> and <b>Adjusted R$^2$</b>.\n",
    "\n",
    "$C_p$ estimate of the test MSE is given as:\n",
    "\n",
    "$$C_p = \\frac{1}{n}(RSS + 2d\\widehat{\\sigma}^2)$$\n",
    "\n",
    "where $d$ is the number of predictors and $\\widehat{\\sigma}^2$ is the estimate of the variance of error $\\epsilon$ associated with each response in the linear model given as:\n",
    "\n",
    "$$Y = \\beta_0 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p + \\epsilon$$\n",
    "\n",
    "The penality added is $2d\\widehat{\\sigma}^2$, which increases as the number of predictors increases. <b>If $\\widehat{\\sigma}^2$ is an unbiased estimator of $\\sigma^2$, then $C_p$ is an unbiased estimate of test MSE.</b> Finally,  model with the lowest value of $C_p$ is chosen.\n",
    "\n",
    "The <b>AIC</b> criterion is used for the models fit by maximum likelihood. AIC is given as:\n",
    "\n",
    "$$C_p = \\frac{1}{n\\widehat{\\sigma}^2}(RSS + 2d\\widehat{\\sigma}^2)$$\n",
    "\n",
    "where the terms has the same meaning as explained above. $C_p$ and AIC are proportional to each other.\n",
    "\n",
    "<b>BIC</b> is derived from a Bayesian point of view. For a least square model, BIC is given as:\n",
    "\n",
    "$$BIC = \\frac{1}{n}(RSS + log(n)d\\widehat{\\sigma}^2)$$\n",
    "\n",
    "As instead of 2, BIC has a factor of $log(n)$, for values of $n>7$, BIC ends up penalizing models with higher predictors more and hence results in a selection of smaller models than $C_p$.\n",
    "\n",
    "The <b>Adjusted R$^2$</b> statistic is another measure for model selection. $R^2$ is defined as $1 - \\frac{RSS}{TSS}$, where RSS is <b>Residual sum of squares</b> and TSS is <b>Total sum of squares</b>. TSS is given as $\\sum(y_i - \\bar{y})^2$. For a least squares model with $d$ predictors, Adjusted R$^2$ statistic is given as:\n",
    "\n",
    "$$Adjusted \\ R^2 = 1 - \\frac{RSS/(n-d-1)}{TSS/(n-1)}$$\n",
    "\n",
    "A <b>large value of adjusted R$^2$ indicates smaller test error</b>. The concept behind adjusted R$^2$ is the fact that once the model is saturated (in terms of predictor), adding more predictors will only decrease RSS by a lower value but as the denominator $(n-d-1)$ decreases by a larger proportion, the value $\\frac{RSS}{n-d-1}$ will overall increase, decreasing the value of adjusted R$^2$.\n",
    "\n",
    "Here the expression for AIC, BIC and $C_p$ is given for linear model fit with least squares. The general expression for them can be computed as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation and Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of computing the above parameters, we can directly estimate the test error by cross-validation methods. This process has an advantage comared to the methods discussed above as it provides a direct estimate of the test error and makes fewer assumption about the underlying model. It can also be used in the cases when it is hard to identify the number of predictors in the model or estimation of error variance $\\sigma^2$ is difficult.\n",
    "\n",
    "<b>As the estimate of test MSE depends on the validation set or the method of cross-validation(the estimate will be different fro 5-fold and 10-fold cross-validation), for the models with slight variance in the estimated test MSE across the variation in the number of predictors, there is a chance that if the validation process is repeated with different split of data, the model selection will change.</b> For this reason, we follow <b>one-standard-error rule</b>. The rule suggests that the lowest point in the curve for the estimated test MSE is found, and eventually, the smallest model (model with smallest number of predictors) for which the estimated test error is within one standard error away from the lowest point is selected as the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Shrinkage Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative to subset selection methods, a model containing all the <b>$p$ predictors</b> can be fit using a technique that <b>constrains</b> or <b>regularizes</b> the coefficient estimates (or <b>shrinks</b> the coefficeint estimates towards 0). Two best known techniques for shrinking the coefficient estimates towards 0 are: <b>ridge regression</b> and the <b>lasso</b>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.1 Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a least squares fitting, the parameters of the model is estimated by minimizing\n",
    "\n",
    "$$RSS = \\sum_{i=1}^{n} \\bigg( y_i - (\\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij})\\bigg)^2$$\n",
    "\n",
    "In <b>ridge regression</b>, coefficients are estimated by minimizing the following term instead:\n",
    "\n",
    "$$RSS + \\lambda \\sum_{j=1}^{p} \\beta_j^2 = \\sum_{i=1}^{n} \\bigg( y_i - (\\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij})\\bigg)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2$$\n",
    "\n",
    "where $\\lambda \\geq 0$ is called as the <b>tuning parameter</b> and is determined separately. In ridge regression, the model fits the data well by minimizing RSS and <b>shrinking penalty</b> ($\\lambda \\sum_{j=1}^{p} \\beta_j^2$). Shrinking penalty will be small when $\\beta_i$s will be close to 0 and hence minimizing this has the effect of shrinking the coefficient estimates towards 0. When the <b>tuning parameter</b> $\\lambda$ is 0, ridge regression will give the least squares estimates. For the larger value of $\\lambda$, the impact of shrinkage penalty increases and hence the coefficient estimates approach closer to 0. We can generate different sets of coefficient estimates for different values of $\\lambda$. The best estimate can be chosen using several cross-validation methods. It is to be noted that the <b>shrinkage penalty is not applied to the intercept</b>.\n",
    "\n",
    "When value of $\\lambda$ is very large, all the ridge regression coefficients approach 0, giving us the <b>null model</b> (model whic contain only intercept and no predictors). In aggregate, the ridge regression coefficients tend to decrease as $\\lambda$ increases but some of the individual coefficients may increase. This phenomenon is shown in the left hand side figure below.\n",
    "\n",
    "<img src=\"images/ridge.PNG\"  width=\"1000px\">\n",
    "\n",
    "Instead we can plot the ridge coefficients against the ratio of <b>l$_2$ norms</b> of the ridge coefficients vector and least squares regression coefficients vector (intercept is excluded). Its value ranges from 1 (when $\\lambda = 0$, the ridge coefficients and least squares coefficients are same) to 0 (when $\\lambda = \\infty$, ridge coefficients aprroach 0). This plot is shown in the right hand side figure above.\n",
    "\n",
    "The standard least squares coefficients are <b>scale invariant</b>. This means that multiplying a predictor by a constant $c$, has an effect of scaling down the least squares coefficients by $\\frac{1}{c}$. In contrast, when multiplying a predictor by $c$, the ridge coefficient changes substantially. Hence, <b>it is best to apply the ridge regression after standardizing the predictors</b> as:\n",
    "\n",
    "$$\\tilde{x_{ij}} = \\frac{x_{ij}}{\\sigma_j} = \\frac{x_{ij}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (x_{ij} - \\bar{x_j})^2}}$$\n",
    "\n",
    "Here the denominator is the <b>estimated standard deviation of the $j$th predictor</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why Does Ridge Regression Improve Over Least Squares?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ridge regression, as $\\lambda$ increases, the fliexibility of the model decreases, leading to low variance and hence increased bias. The model with least squares coefficient estimates(corresponding to $\\lambda = 0$), has high variance but there is less bias. But as $\\lambda$ increases, the shrinkage of the model coefficients leads to a substantial decrease in the model variance with a slight increase in the bias. As $\\lambda$ increases further, the decrease in variance is surpassed by the increase in bias. Hence, <b>minimum MSE is achieved at an intermediate value of $\\lambda$</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.2 The Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ridge regression has one disadvantage though. The ridge regression model will always have all the $p$ predictors (though it will shrink their coefficeints towards 0) until and unless $\\lambda$ is $\\infty$. It may not be a problem when prediction accuracy is concerned but for the <b>model interpretability</b>, it can create a challenge.\n",
    "\n",
    "The <b>lasso</b> is an alternative for ridge regression that overcomes this disadvantage. The lasso coefficients minimize\n",
    "\n",
    "$$RSS + \\lambda \\sum_{j=1}^{p} |\\beta_j| = \\sum_{i=1}^{n} \\bigg( y_i - (\\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij})\\bigg)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|$$\n",
    "\n",
    "The only difference between lasso and ridge regression is that the lasso uses $l_1$ penalty instead of an $l_2$ penalty used in the ridge regression. Lasse shrinks some of the coefficients to exactly equal to 0 for larger values of $\\lambda$ and hence performing <b>variable selection</b>. As a result lasso can provide <b>sparse</b> models (model with fewer predictors) which are easy to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Another Formulation for Ridge Regression and the Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be shown that the lasso and ridge regression coefficient estimates solve the problems\n",
    "\n",
    "$$minimize_{\\beta}\\bigg [ \\sum_{i=1}^{n} \\bigg( y_i - (\\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij})\\bigg)^2 \\bigg] \\ \\ \\   subject \\ to \\  \\sum_{j=1}^{p} |\\beta_j| \\leq s$$\n",
    "\n",
    "$$minimize_{\\beta}\\bigg [ \\sum_{i=1}^{n} \\bigg( y_i - (\\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij})\\bigg)^2 \\bigg] \\ \\ \\   subject \\ to \\  \\sum_{j=1}^{p} \\beta_j^2 \\leq s$$\n",
    "\n",
    "respectively. This can be interpreted as: for every value of $\\lambda$ there is some $s$ that the above equations will give the lasso and ridge regression coefficients. \n",
    "\n",
    "For 2 predictor case, the above equation can be interpreted as: <b>the lasso and ridge coefficient estimates have the smallest RSS out of all points that lie within $|\\beta_1| + |\\beta_2| \\leq s$ and $\\beta_1^2 + \\beta_2^2 \\leq s$ respectively</b>. Similarly, the subset selection problem can be summarized as:\n",
    "\n",
    "$$minimize_{\\beta}\\bigg [ \\sum_{i=1}^{n} \\bigg( y_i - (\\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij})\\bigg)^2 \\bigg] \\ \\ \\   subject \\ to \\  \\sum_{j=1}^{p} I(\\beta_j \\neq 0) \\leq s$$\n",
    "\n",
    "For large values of $p$, solving the subset selection problem is infeasible as we need to consider all the possible combinations ${p\\choose s}$ for this. Lasso and ridge regression can be viewed as a simplified form for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Variable Selection Property of the Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lasso.PNG\"  width=\"800px\">\n",
    "\n",
    "Above figure explains why estimated lasso coefficients can be 0. In the above figure, the least squares solution is marked as $\\widehat{\\beta}$ and the lasso and ridge regression constraints are marked as diamond and circle respectively. The ellipses that are centered around $\\widehat{\\beta}$ represents regions of constant RSS. As the ellipse expands away from least squares regression coefficient, RSS increases. <b>The lasso and ridge regression coefficients are given by the points at which an ellipse contacts the region defined by the constraint.</b> As the contour of ridge regression constraint does not have any sharp edges, an ellipse will never cross it at one of the axises and hence will never give a single 0 coefficient value. But, as the lasso constraint contour has corners at each of the axis, the ellipse will often intersect the constraint at one of the axes giving the 0 value for the coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing the Lasso and Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neither ridge regression nor the lasso will universally dominate the other. The lasso will perform better in the setting where a relatively samll number of predictors are statistically significant. If all the predictors have coefficients of equal magnitude, ridge regression will perform better. But the number of predictors that are related to the response is not known in advance, we need to cross-validate the model and then choose the best one.\n",
    "\n",
    "<b>Ridge regression shrinks every dimenion of the data by same proportion. Lasso shrinks every coefficient towards 0 by same amount and the smaller coefficients are shrinken all the way to 0.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bayesian Interpretation for Ridge Regression and the Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Bayesian viewpoint of regression assumes that the coefficient vector $\\beta$ has some <b>prior distribution</b>,  $p(\\beta)$. Then the likelihood of data can be represented as $f(Y \\ | \\ X, \\beta)$. Multiplying the prior distribution from the likelihood gives us <b>posterior distribution</b>:\n",
    "\n",
    "$$p(\\beta | X,Y) \\propto f(Y \\ | \\ X, \\beta)\\   p(\\beta|X) = f(Y \\ | \\ X, \\beta)\\ p(\\beta)$$\n",
    "\n",
    "Let us assume that the prior distribution of $\\beta$ is given by the density function $g$, then $p(\\beta) = \\prod_{j=1}^{p} g(\\beta_j)$.\n",
    "\n",
    "\n",
    " - If $g$ is a <b>Gaussian distribution</b> with mean 0 and standard deviation some function of $\\lambda$, the <b>posterior mode</b> of $\\beta$ gives the ridge regression coefficient. The ridge regression coefficient is also the posterior mean.\n",
    " \n",
    " \n",
    " - If $g$ is a <b>double exponential(Laplace) distribution</b> with mean 0 and scale parameter some function of $\\lambda$, the <b>posterior mode</b> of $\\beta$ gives the lasso coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.3 Selecting the Tuning Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To design a better model, we need to select proper <b>tuning parameter</b> $\\lambda$ or the <b>constraint</b> $s$. Cross-validation provides a concrete process to handle this. We can fit the models for different values of $\\lambda$, and, by the process of cross-validation, select the best of it. Finally the model is fitted for the selected value of $\\lambda$ using all the available observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Dimension Reduction Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of performing a least squares regression on all the $p$ predictors, we can <b>transform</b> the predictors and then fit a least squares model on the transformed variables. Let the transformed variables be $Z_1, Z_2, ..., Z_M$, where $M < p$, where each of $Z_m$s is a linear combination of predictors $X_1, X_2, ..., X_p$.i.e.\n",
    "\n",
    "$$Z_m = \\sum_{j=1}^{p} \\phi_{jm}X_j$$\n",
    "\n",
    "We can then fit the least squares regression model as $Z_m$s as the predictors:\n",
    "\n",
    "$$Y = \\theta_0 + \\theta_1 Z_1 + ... + \\theta_M Z_M + \\epsilon$$\n",
    "\n",
    "If the transformation constants are chosen wisely, then the dimensionality reduction approach can outperform plain least squares models. The term <b>dimensionality reduction</b> comes form the fact that instead of predicting $p+1$ coefficients, we only need to predict $M+1$ coefficients now. The least square coefficients can be derived from the transformation constants and the coefficients of the dimensionality reduction model as:\n",
    "\n",
    "$$\\beta_j = \\sum_{m=1}^{M} \\theta_m \\phi_{jm} $$\n",
    "\n",
    "Dimensionality reduction reduces the variance of the model. It works in two steps: first the transformed predictors $Z_1, Z_2, ... , Z_M$ are obtained and then the model is fitted using these $M$ predictors. Dimesionality reduction can be achieved by: <b>principal componenets</b> and <b>partial least squares</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.1 Principal Components Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Principal Component Analysis (PCA)</b> can be used to derive a low-dimensional set of features from a large set of variables. The <b>first principal component</b> is the direction of the data along which the observations <b>vary the most</b>. For a two variable case, the linear combination of the variables can be written as $\\phi_{11} X_1 + \\phi_{21} X_2$, where $\\phi_{11}^2 + \\phi_{21}^2 = 1$. The idea of PCA is to find the linear combination for which the variance is maximum out of all the possible ones. It is necessary to put a constraing of $\\phi_{11}^2 + \\phi_{21}^2 = 1$, as otherwise, variance can simply be increased by increasing the $\\phi$s. <b>First principal componenet</b> can also be described as the line which is as close as possible to the data.\n",
    "\n",
    "The <b>second principal component</b> can be described as the linear combinations of variables that is <b>uncorrelated to the first principal component $Z_1$</b> (this means that it should be <b>orthogonal</b> to the fisrt principal component) and have the largest variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Principal Components Regression Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>principal components regression (PCR)</b> approach constructs the first $M$ principal components first and then fit a least squares linear regression model on them. The basic idea behind the PCR is the fact that the <b>directions in which the predictors $X_1, X_2, ..., X_p$ shows the most variation are the directions that should be associated with the response $Y$</b>. Hence PCR can give better results over least square linear regression model by <b>mitigating overfitting</b>. As the number of principal components in a model is increased, the bias will decrease and the variance will increase. Performing PCR with appropriate number of principal components can improve the performance of the model compared to the least squares linear regression model.\n",
    "\n",
    "PCR is a method of performing regression using $< p$ predictors but it is not a feature selection method as all the $p$ features are contained in $M$ principal components and hence <b>PCR is closely related to ridge regression</b>. One can even think of ridge regression as a continuous version of PCR.\n",
    "\n",
    "In PCR, the desired number of principal components can be chosen by cross-validation. Prior to performing PCR, we need to standardize the individual variables as if we do not do so, the variables with higher variance will tend to affect the principal components obtained and hence will have an effect on the PCR model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.2 Partial Least Squares "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PCR, the linear combinations or directions are identified in an <b>unsupervised way</b> (as the response $Y$ is not used to determine the principal components). Hence, it may be the case that the directions that best describe the predictors are the ones which best describe the response. In this case, the performance will PCR will suffer.\n",
    "\n",
    "<b>Partial least squares (PLS)</b> is a <b>supervised</b> alternative to PCR. PLS identifies the new features (dimensionality reduction) in a supervised way (it makes the use of response $Y$ in the identification of the new features). In a nut shell, <b>PLS attempts to find directions that explain both the response and the predictors.</b>\n",
    "\n",
    "In PLS, the <b>first principal component</b> is computed by setting the <b>transformation coefficients ($\\phi_{j1}$)</b> as the <b>coefficient of the simple linear regression</b> of response $Y$ onto individual variables $X_j$. Hence, PLS gives the highest weights to the variables that are most strongly related to the response while doing the transformation.\n",
    "\n",
    "Second PLS direction is identified by removing from each of the variables $X$s, the part which is already explained by $Z_1$. This can be done by <b>regressing the each variable on $Z_1$ and then taking the residuals.</b> These residuals are then used to determine the second principal component. This iterative approach is performed $M$ times to identify the PLS components $Z_1, Z_2, ..., Z_M$, which are then used to find the least squares fit of $Y$. $M$ is a tuning parameter, which can be found using cross-validation. We need to <b>standardize the predictors and response</b> before performing PLS. In practice it often performs no better than ridge regression or PCR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Considerations in High Dimensions\n",
    "#### 6.4.1 High-Dimensional Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data set which has considerably more features than observations are termed as <b>high-dimensional</b> data set. Classical approaches such as least squares linear regressions are not suitable for this setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.2 What Goes Wrong in High Dimensions? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a high-dimensional setting, no matter whether there is a true relationship between the features and the response, classical approaches will lead to the models which will be a perfect fit to the data (residuals are 0). This is problematic as these perfect fit will lead to <b>overfitting</b>. For example, when $p \\approx n$ or $p > n$, the least square line will is too <b>flexible</b> and hence overfits the data. Hence, we need to take extra care while analyzing high-dimensional data sets and always evaluate model performance on an independent test set. The various approaches to predict test MSE, such as $C_p$, BIC, AIC and adjusted $R^2$ does not perform well in high-dimensional settings and hence alternative approaches, best suited for high-dimensional settings are required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.3 Regression in High Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various approaches that are described for fitting a <b>less flexible</b> least square models (subset selection, stepwise selection, ridge regression, lasso) perorm well in high-dimensional setting. While using these models in high-dimensional setting we should always follow these points:\n",
    "\n",
    " - <b>Regularization</b> or <b>shrinkage</b> plays a key-role in high dimensional setting \n",
    " \n",
    " \n",
    " - Selection of appropriate <b>tuning parameter</b> is key for good prediction accuracy\n",
    " \n",
    " \n",
    " - Test error will increase as the dimensionality of the problem increases, until and unless the added features are truely associated with the response (also called as <b>curse of dimensionality</b>).\n",
    " \n",
    "Hence, more extensive data collection can lead to overfitted model without much improvement in the prediction accuracy until and unless new features are associated with response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.4 Interpreting Results in High Dimensions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In high-dimensional setting, the problem of <b>multi-collinearity</b> (variables in the regression might be correlated to each other) is a huge problem and hence we can not truly identify the variables which are predictive of the outcome. This may lead to <b>overstating</b> the results as the predictors included in the model will not be the only one which can predict the response efficiently. There may be a case that if we design a model based on a fresh training data, we may lead to get a different set of significant predictors but the model accuracy will remain unchanged.\n",
    "\n",
    "Apart from this, it would be misleading to report traditional measures for the goodness of fit (p-value, $R^2$ statistic) on the <b>training data</b> as the statistic for model performance as it is easy to obtain 0 residuals. Instead, we can report the results obtained on an independent test set or may perform cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
