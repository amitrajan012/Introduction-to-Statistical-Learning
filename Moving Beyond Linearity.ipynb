{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.text_cell_render { /* Customize text cells */\n",
       "font-family: 'Times New Roman';\n",
       "font-size:1.3em;\n",
       "line-height:1.4em;\n",
       "padding-left:1.5em;\n",
       "padding-right:1.5em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "div.text_cell_render { /* Customize text cells */\n",
    "font-family: 'Times New Roman';\n",
    "font-size:1.3em;\n",
    "line-height:1.4em;\n",
    "padding-left:1.5em;\n",
    "padding-right:1.5em;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Moving Beyond Linearity</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lineaer models have its limitations in terms of predictive power. Linear models can be extended simply as:\n",
    "\n",
    " - <b>Polynomial regression</b> extends linear regression by adding extra higher order predictors (predictors rasied to higher order powers).\n",
    " \n",
    " \n",
    " - <b>Step functions</b> cut the range of a variable into $K$ distinct regions in order to produce a qualitative variable.\n",
    " \n",
    " \n",
    " - <b>Regression splines</b> is the extension of polynomial regression and step functions. It divides the range of predictor $X$ into $K$ distinct regions and within each region a polynomial function is fit to the data.\n",
    " \n",
    " \n",
    " - <b>Smoothing splines</b>\n",
    " \n",
    " \n",
    " - <b>Local regression</b>\n",
    " \n",
    "  \n",
    " - <b>Generalized additive models</b>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A standard linear regression model\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$$\n",
    "\n",
    "can be replaced by a more generic polynomial function\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + ... + \\beta_d x_i^d + \\epsilon_i$$\n",
    "\n",
    "This approach is known as <b>polynomial regression</b> and for large enough values of $d$, it can produce a highly non-linear curve. It is highly unusual to use $d$ greater than 3 or 4. The given model parameters can easily be estimated using linear least squares linear regression procedure. Similarly, polynomial functions can be modeled with the <b>logistic regression</b> as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Step Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial regression gives a fit that is more <b>global</b> in nature. In <b>step functions</b>, we divide the range of $X$ into <b>bins</b> and fit a different constant in each bin. We can create $K$ <b>cutpoints</b> $c_1, c_2, ..., c_K$ in the range of $X$, and then can construct $K+1$ new <b>categorical</b> variables as:\n",
    "\n",
    "$$C_i(X) = I(c_i \\leq X < c_{i+1})$$\n",
    "\n",
    "where $I(.)$ is an <b>indicator function</b> which returns 1 if the condition is true and 0 oterwise. For any value of $X$, $C_0(X) + C_1(X) + ... + C_K(X) = 1$, as only one value will be 1 for each $X$. We can then fit a linear least squares model to fit $C_1(X), C_2(X),...,C_K(X)$ as predictors. We need to omit one predictor as there will be intarcept too. The linear model is given as:\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 C_1(x_i) + \\beta_2 C_2(x_i) + ... + \\beta_K C_K(x_i) + \\epsilon_i$$\n",
    "\n",
    "$\\beta_0$ is a response for $X<c_1$. The response for $c_j \\leq X < c_{j+1}$ is $\\beta_0 + \\beta_j$. Hence, $\\beta_j$ represents the average increase in the response for $X$ in $c_j \\leq X < c_{j+1}$ relative to $X < c_1$. Logistic regression model can be fitted in the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Basis Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial and piecewise-constant regression models are special cases of a <b>basis function</b> approach for regression. In basis function approach, we use a family of functions to transform $X$ and instead of fitting a linear model in $X$, we fit the transformed predictors as:\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 b_1(x_i) + \\beta_2 b_2(x_i) + ... + \\beta_K b_K(x_i) + \\epsilon_i$$\n",
    "\n",
    "The basis functions are fixed and known. For polynomial regression, the basis functions are $b_j(x_i) = x_i^j$. For piecewise constant functions, they are $b_j(x_i) = I(c_j \\leq x_i < c_{j+1})$. As in basis functions approach linear model is fitted on the transformed variables, all the inference tools for linear models can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Regression Splines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression splines are flixible class of basis functions that extend upon polynomial and piecewise constant regression approaches.\n",
    "\n",
    "#### 7.4.1 Piecewise Polynomials\n",
    "\n",
    "<b>Piecewise polynomial regression</b> fits separate low-degree polynomials over different regions of $X$. For example, a piecewise squared polynomial fits squared regression model of the form\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i$$\n",
    "\n",
    "where the coefficients $\\beta_0, \\beta_1, \\beta_2$ differs in different parts of the range of $X$. The points where the coefficients change are called <b>knots</b>. Each of the polynomial functions can be fit using least square methods. Increasing the number of knots will give a more flexible piecewise polynomial.\n",
    "\n",
    "#### 7.4.2 Constraints and Splines\n",
    "\n",
    "By using piecewise polynomial regression, the fitted curve on the data may have a <b>discontinuity at the knots</b> or we can say that the fitted curve is too flexible. Instead, we can fit a piecewise polynomial under the constraint that the fitted curve must be continuous. We can further add more constraints, such as, both the first and second derivatives of the piecewise polynomials must be continuous. <b>Each added constraint frees up one degree of freedom. and hence reducing the complexity of the resulting piecewise polynomial fit</b>. Hence by imposing three constraints of continuity, continuity of the first and second derivative, we reduce the degree of freedom of model by 3.\n",
    "\n",
    "A piecewise cubic polynomial function with three constraints(continuity, continuity of the first and second derivative) is called as <b>cubic spline</b>. The degree of freedom of cubic spline is $K+4$, where $K$ is the <b>number of knots</b>. It can be explained as: The left(or right) end of the polynomial has a degree of freedom 4(as we have to estimate 4 coefficients or parameters to fit a cubic spline). Each additional knot adds one parameter (as three imposed constraints leave one free parameter) and hence making a total of $K+4$ parameters for $K$ knots. In general, a <b>degree-d spline</b> is a piecewise degree-d polynomial with continuity in derivatives upto degree $d-1$ at each knot.\n",
    "\n",
    "#### 7.4.3 The Spline Basis Representation\n",
    "\n",
    "A cubic spline with $K$ knots can be modeled as:\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 b_1(x_i) + \\beta_2 b_2(x_i) + ... + \\beta_{K+3} b_{K+3}(x_i) + \\epsilon_i$$\n",
    "\n",
    "First of all, the equation can be interpreted as: the degree of freedom of a cubic spline is $K+4$ and hence we have to estimate $K+4$ parameters. After composing the equation, we need to formulate the <b>basis functions</b> $b_1, b_2, ..., b_{K+3}$. As explained above, a cubic spline can be iterpreted as a polynomial function where left(or right) end has a degree of freedom 4 (as we need to fit a cubic polynomial without any constraint) giving the first three basis functions as $x, x^2$ and $x^3$. Then we have to add one degree of freedom (parameter) per knot, with the constraints of continuity and continuity of the first and second derivatives. This behaviour can be captured by adding one <b>truncated power basis function</b> per knot, which is given as:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "  h(x, \\xi) = (x - \\xi)^3_+ = \\left\\{\n",
    "  \\begin{array}{@{}ll@{}}\n",
    "    y(x - \\xi)^3, & \\text{if}\\ x > \\xi \\\\\n",
    "    0, & \\text{otherwise}\n",
    "  \\end{array}\\right.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $\\xi$ is the knot. Adding $\\beta_ih(x, \\xi)$ will lead to discontinuity only in the third derivative at $\\xi$. Hence to fit a cubic spline to a data set with $K$ knots, we need to perform least squares regression to estimate an intercept and $3+K$ parameters for $X, X^2, h(X, \\xi_1), h(X, \\xi_2), ..., h(X, \\xi_K)$, where $\\xi_1, \\xi_2, ..., \\xi_K$ are the knots.\n",
    "\n",
    "Cubic splines have higher variance at the ends. A <b>natural spline</b> adds additional <b>boundary constraints</b>(requirement of being linear at boundaries, reducing 2 degree of freedom at each boundary) and hence reduce the variance, producing more stable estimates at boundaries.\n",
    "\n",
    "#### 7.4.4 Choosing the Number and Locations of the Knots\n",
    "\n",
    "The regression spline is most flexible in the regions which have highest number of knots. One approach is to place higher number of knots in the regions where we feel that the function might vary the most. In practice, it is common to place knots in a uniform fashion. The number of knots can be decided by analyzing the curve visually or by cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
