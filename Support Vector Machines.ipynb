{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.text_cell_render { /* Customize text cells */\n",
       "font-family: 'Times New Roman';\n",
       "font-size:1.3em;\n",
       "line-height:1.4em;\n",
       "padding-left:1.5em;\n",
       "padding-right:1.5em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "div.text_cell_render { /* Customize text cells */\n",
    "font-family: 'Times New Roman';\n",
    "font-size:1.3em;\n",
    "line-height:1.4em;\n",
    "padding-left:1.5em;\n",
    "padding-right:1.5em;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Support Vector Machines</center></h1>\n",
    "\n",
    "<b>Support vector machine</b> is a generalization of a simple and intutive classifier called the <b>maximal margin classifier</b>. Maximal margin classifier has a limitation that it can be only applied to a data set whose classes are seperated by a linaer boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Maximal Margin Classifier\n",
    "#### 9.1.1 What Is a Hyperplane?\n",
    "\n",
    "In a $p$-dimensional space, a <b>hyperplane</b> is a flat affine subspace of $p-1$ dimensions. In two dimensions, the hyperplane is a line and can be given as:\n",
    "\n",
    "$$\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 = 0$$\n",
    "\n",
    "for parameters $\\beta_0, \\beta_1, \\beta_2$. In a $p$ dimensional setting, the hyperplane can be given as:\n",
    "\n",
    "$$\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p = 0$$\n",
    "\n",
    "If a point $X = (X_1, X_2, ..., X_p)^T$ in the space satisfies the above equation, the point $X$ lies on the hyperplane.\n",
    "\n",
    "If $X$ satisfies $\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p > 0$, $X$ lies to one side of the hyperplane. On the other hand, if $\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p < 0$, $X$ lies on the other side of the hyperplane. Hence, a hyperplane divides a $p$-dimensional space into <b>two halves</b>.\n",
    "\n",
    "#### 9.1.2 Classification Using a Separating Hyperplane\n",
    "\n",
    "Classification can be done by using the concept of seperating hyperplanes. Suppose there exists a seperating hyperplane whihc divides the data set perfectly into two classes, labeled as $y_i=1, y_i=-1$. Separating hyperplane can be defined as:\n",
    "\n",
    "$$\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p > 0 \\ \\ if \\ \\ y_i= 1$$\n",
    "\n",
    "$$\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p < 0 \\ \\ if \\ \\ y_i= -1$$\n",
    "\n",
    "and can be combined as:\n",
    "\n",
    "$$y_i(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p) > 0; \\forall \\  i=1,2,..,n$$\n",
    "\n",
    "If a separating hyperplane exists, it can be used to construct a classifier and an observation can be classified depending on which side of the hyperplane it is located. We can also use the <b>magnitude</b> of the distance of the observation from the hyperplane to decide the confidence of the classification. More the distance, the more is the confidence about the classification of the observation. A classifier based on a separating hyperplane leads to a <b>linear decision boundary</b>.\n",
    "\n",
    "#### 9.1.3 The Maximal Margin Classifier\n",
    "\n",
    "If the data set can be separated by a separating hyperplane, an infinity number of such hyperplanes exist. Hence, we need to devise a way to select which of the infinite possible hyperplanes to use.\n",
    "\n",
    "A natural choice is the <b>maximal margin classifier</b> (also called as <b>optimal separating hyperplane</b>). It is the hyperplane which is farthest from the training observations. <b>Margin</b> is the smallest amongst the perpendicular distance of all the observations from the hyperplane. Maximal margin classifier is the hyperplane for which the <b>margin is maximum</b>. Maximal margin classifiers are often successful but they can lead to <b>overfitting</b> for large values of $p$.\n",
    "\n",
    "<b>Support vectors</b> are the observations which are on the width of the classifier. If these points are moved, the maximal margin hyperplane will move as well, as these are the points which decide the end of the regions (margin) around the maximal margin classifier. Maximal margin hyperplane depends only on the support vectors. Displacement of other observations does not affect the maximal margin hyperplane, until and unless the observation crosses the boundary set by the margin.\n",
    "\n",
    "#### 9.1.4 Construction of the Maximal Margin Classifier\n",
    "\n",
    "Given a set of $n$ training observations $x_1, x_2, ..., x_n$ in a $p$-dimensional space having the class labels $y_1, y_2, ..., y_n \\in \\{-1, 1\\}$, maximal margin hyperplane is the solution of the optimization problem:\n",
    "\n",
    "$$maximize_{\\beta_0, \\beta_1, ..., \\beta_p} M$$\n",
    "\n",
    "$$subject \\ to \\ \\sum_{j=1}^{p} \\beta_j^2 = 1,$$\n",
    "$$y_i(\\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + ... + \\beta_p X_{ip}) \\geq M; \\forall \\  i=1,2,..,n$$\n",
    "\n",
    "The second condition simply ensures that all the observations are on or beyond the margin $M$, given $M$ is positive, and on the correct side of the classification. Due to the first constraint, the perpendicular distance from the $i$th observation to the hyperplane is given by $y_i(\\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + ... + \\beta_p X_{ip})$. Hence, $M$ represents the <b>margin</b> of the hyperplane and the optimization problem chooses $\\beta_0, \\beta_1, ..., \\beta_p$ that maximizes $M$.\n",
    "\n",
    "#### 9.1.5 The Non-separable Case\n",
    "\n",
    "Maximal margin classifier can be obtained if and only if the seperating hyperplane exists. The generalization of the maximal margin classifier to accomodate the <b>non-separable</b> classes is known as the <b>support vector classifier</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Support Vector Classifiers\n",
    "\n",
    "#### 9.2.1 Overview of the Support Vector Classifier\n",
    "\n",
    "The maximal margin classifiers can be sensitive to individual observations. Sometimes, adding a single observation in the data set, can lead to dramatic change in the separating hyperplane. The sensitivity and the low margin for a maximal margin classifier may suggest that the maximal margin classifier has <b>overfit</b> the training data. So, sometimes we may be willing to consider a classifier based on a hyperplane that does not perfectly separate the two classes, and hence, will be <b>more robust (or less sensitive to individual observations)</b> and will give better results for the unseen data points.\n",
    "\n",
    "<b>Support vector classifiers (soft margin classifiers)</b> misclassify a few observations for the sake of robustness and better results. In a soft margin classifier, some of the observations can lie on the wrong side of the margin or even on the wrong side of the hyperplane. Observations that are on the wrong side of the hyperplane are misclassified by the support vector classifier.\n",
    "\n",
    "#### 9.2.2 Details of the Support Vector Classifier\n",
    "\n",
    "Support vector classifier is the solution of the optimization problem which is given as:\n",
    "\n",
    "$$maximize_{\\beta_0, \\beta_1, ..., \\beta_p, \\epsilon_1, \\epsilon_2, ..., \\epsilon_n} M$$\n",
    "\n",
    "$$subject \\ to \\ \\sum_{j=1}^{p} \\beta_j^2 = 1,$$\n",
    "$$y_i(\\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + ... + \\beta_p X_{ip}) \\geq M(1 - \\epsilon_i); \\forall \\  i=1,2,..,n,$$\n",
    "$$\\epsilon_i \\geq 0, \\ \\sum_{i=1}^{n}\\epsilon_i \\leq C$$\n",
    "\n",
    "where $C$ is a non-negative tuning parameter. Here $\\epsilon_1, ..., \\epsilon_n$ are <b>slack variables</b> and allow individual observations to be on the wrong side of the margin or the hyperplane. For a test observation $x^*$, we simply classify it based on the sign of $f(x^*) = \\beta_0 + \\beta_1 x_1^* + ... + \\beta_p x_p^*$.\n",
    "\n",
    "The slack variable $\\epsilon_i$ depicts the location of $i$th observation relative to the margin and the hyperplane. If $\\epsilon_i = 0$, the observation <b>lies on the correct side of the margin</b>. If $\\epsilon_i > 0$, the observation is on the <b>wrong side of the margin</b> and if $\\epsilon_i > 1$, the observation is on the <b>wrong side of the hyperplane</b>.\n",
    "\n",
    "The hyperparamter $C$ amounts for the budget for the amount by which the margin can be violated by $n$ observations. If $C=0$, there is no budget for the violation of the margin and hence the support vector classifier turns into maximal margin classifier. For $C>0$, no more than $C$ observations can be on the wrong side of the hyperplane (as $\\epsilon_i > 1$ for the observation to be on the wrong side of the hyperplane). If $C$ increases, the margin will widen and if it decreases, the margin will become narrow. $C$ is chosen via <b>cross-validation</b>. When $C$ is <b>small</b>, the margin narrows and hence the classifier fits more colsely to the data, giving a <b>high variance</b>. For <b>larger</b> $C$, the margin is wider and hence fitting the data less hard and obtaining a classifier whihc is <b>more biased</b> but have <b>low variance</b>.\n",
    "\n",
    "The observations that lie on the correct side of the margin do not affect the hyperplane. The observations that either lie on the margin or that violate the margin affect the hyperplane and hence are known as <b>support vectors</b>. For larger value of $C$, we will get a more number of support vectors and hence the hyperplane will depend on large number of observations, making the model low in variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
