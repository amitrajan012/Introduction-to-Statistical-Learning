{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.text_cell_render { /* Customize text cells */\n",
       "font-family: 'Times New Roman';\n",
       "font-size:1.3em;\n",
       "line-height:1.4em;\n",
       "padding-left:1.5em;\n",
       "padding-right:1.5em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "div.text_cell_render { /* Customize text cells */\n",
    "font-family: 'Times New Roman';\n",
    "font-size:1.3em;\n",
    "line-height:1.4em;\n",
    "padding-left:1.5em;\n",
    "padding-right:1.5em;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Support Vector Machines</center></h1>\n",
    "\n",
    "<b>Support vector machine</b> is a generalization of a simple and intutive classifier called the <b>maximal margin classifier</b>. Maximal margin classifier has a limitation that it can be only applied to a data set whose classes are seperated by a linaer boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Maximal Margin Classifier\n",
    "#### 9.1.1 What Is a Hyperplane?\n",
    "\n",
    "In a $p$-dimensional space, a <b>hyperplane</b> is a flat affine subspace of $p-1$ dimensions. In two dimensions, the hyperplane is a line and can be given as:\n",
    "\n",
    "$$\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 = 0$$\n",
    "\n",
    "for parameters $\\beta_0, \\beta_1, \\beta_2$. In a $p$ dimensional setting, the hyperplane can be given as:\n",
    "\n",
    "$$\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p = 0$$\n",
    "\n",
    "If a point $X = (X_1, X_2, ..., X_p)^T$ in the space satisfies the above equation, the point $X$ lies on the hyperplane.\n",
    "\n",
    "If $X$ satisfies $\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p > 0$, $X$ lies to one side of the hyperplane. On the other hand, if $\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p < 0$, $X$ lies on the other side of the hyperplane. Hence, a hyperplane divides a $p$-dimensional space into <b>two halves</b>.\n",
    "\n",
    "#### 9.1.2 Classification Using a Separating Hyperplane\n",
    "\n",
    "Classification can be done by using the concept of seperating hyperplanes. Suppose there exists a seperating hyperplane whihc divides the data set perfectly into two classes, labeled as $y_i=1, y_i=-1$. Separating hyperplane can be defined as:\n",
    "\n",
    "$$\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p > 0 \\ \\ if \\ \\ y_i= 1$$\n",
    "\n",
    "$$\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p < 0 \\ \\ if \\ \\ y_i= -1$$\n",
    "\n",
    "and can be combined as:\n",
    "\n",
    "$$y_i(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p) > 0; \\forall \\  i=1,2,..,n$$\n",
    "\n",
    "If a separating hyperplane exists, it can be used to construct a classifier and an observation can be classified depending on which side of the hyperplane it is located. We can also use the <b>magnitude</b> of the distance of the observation from the hyperplane to decide the confidence of the classification. More the distance, the more is the confidence about the classification of the observation. A classifier based on a separating hyperplane leads to a <b>linear decision boundary</b>.\n",
    "\n",
    "#### 9.1.3 The Maximal Margin Classifier\n",
    "\n",
    "If the data set can be separated by a separating hyperplane, an infinity number of such hyperplanes exist. Hence, we need to devise a way to select which of the infinite possible hyperplanes to use.\n",
    "\n",
    "A natural choice is the <b>maximal margin classifier</b> (also called as <b>optimal separating hyperplane</b>). It is the hyperplane which is farthest from the training observations. <b>Margin</b> is the smallest amongst the perpendicular distance of all the observations from the hyperplane. Maximal margin classifier is the hyperplane for which the <b>margin is maximum</b>. Maximal margin classifiers are often successful but they can lead to <b>overfitting</b> for large values of $p$.\n",
    "\n",
    "<b>Support vectors</b> are the observations which are on the width of the classifier. If these points are moved, the maximal margin hyperplane will move as well, as these are the points which decide the end of the regions (margin) around the maximal margin classifier. Maximal margin hyperplane depends only on the support vectors. Displacement of other observations does not affect the maximal margin hyperplane, until and unless the observation crosses the boundary set by the margin.\n",
    "\n",
    "#### 9.1.4 Construction of the Maximal Margin Classifier\n",
    "\n",
    "Given a set of $n$ training observations $x_1, x_2, ..., x_n$ in a $p$-dimensional space having the class labels $y_1, y_2, ..., y_n \\in \\{-1, 1\\}$, maximal margin hyperplane is the solution of the optimization problem:\n",
    "\n",
    "$$maximize_{\\beta_0, \\beta_1, ..., \\beta_p} M$$\n",
    "\n",
    "$$subject \\ to \\ \\sum_{j=1}^{p} \\beta_j^2 = 1,$$\n",
    "$$y_i(\\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + ... + \\beta_p X_{ip}) \\geq M; \\forall \\  i=1,2,..,n$$\n",
    "\n",
    "The second condition simply ensures that all the observations are on or beyond the margin $M$, given $M$ is positive, and on the correct side of the classification. Due to the first constraint, the perpendicular distance from the $i$th observation to the hyperplane is given by $y_i(\\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + ... + \\beta_p X_{ip})$. Hence, $M$ represents the <b>margin</b> of the hyperplane and the optimization problem chooses $\\beta_0, \\beta_1, ..., \\beta_p$ that maximizes $M$.\n",
    "\n",
    "#### 9.1.5 The Non-separable Case\n",
    "\n",
    "Maximal margin classifier can be obtained if and only if the seperating hyperplane exists. The generalization of the maximal margin classifier to accomodate the <b>non-separable</b> classes is known as the <b>support vector classifier</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Support Vector Classifiers\n",
    "\n",
    "#### 9.2.1 Overview of the Support Vector Classifier\n",
    "\n",
    "The maximal margin classifiers can be sensitive to individual observations. Sometimes, adding a single observation in the data set, can lead to dramatic change in the separating hyperplane. The sensitivity and the low margin for a maximal margin classifier may suggest that the maximal margin classifier has <b>overfit</b> the training data. So, sometimes we may be willing to consider a classifier based on a hyperplane that does not perfectly separate the two classes, and hence, will be <b>more robust (or less sensitive to individual observations)</b> and will give better results for the unseen data points.\n",
    "\n",
    "<b>Support vector classifiers (soft margin classifiers)</b> misclassify a few observations for the sake of robustness and better results. In a soft margin classifier, some of the observations can lie on the wrong side of the margin or even on the wrong side of the hyperplane. Observations that are on the wrong side of the hyperplane are misclassified by the support vector classifier.\n",
    "\n",
    "#### 9.2.2 Details of the Support Vector Classifier\n",
    "\n",
    "Support vector classifier is the solution of the optimization problem which is given as:\n",
    "\n",
    "$$maximize_{\\beta_0, \\beta_1, ..., \\beta_p, \\epsilon_1, \\epsilon_2, ..., \\epsilon_n} M$$\n",
    "\n",
    "$$subject \\ to \\ \\sum_{j=1}^{p} \\beta_j^2 = 1,$$\n",
    "$$y_i(\\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + ... + \\beta_p X_{ip}) \\geq M(1 - \\epsilon_i); \\forall \\  i=1,2,..,n,$$\n",
    "$$\\epsilon_i \\geq 0, \\ \\sum_{i=1}^{n}\\epsilon_i \\leq C$$\n",
    "\n",
    "where $C$ is a non-negative tuning parameter. Here $\\epsilon_1, ..., \\epsilon_n$ are <b>slack variables</b> and allow individual observations to be on the wrong side of the margin or the hyperplane. For a test observation $x^*$, we simply classify it based on the sign of $f(x^*) = \\beta_0 + \\beta_1 x_1^* + ... + \\beta_p x_p^*$.\n",
    "\n",
    "The slack variable $\\epsilon_i$ depicts the location of $i$th observation relative to the margin and the hyperplane. If $\\epsilon_i = 0$, the observation <b>lies on the correct side of the margin</b>. If $\\epsilon_i > 0$, the observation is on the <b>wrong side of the margin</b> and if $\\epsilon_i > 1$, the observation is on the <b>wrong side of the hyperplane</b>.\n",
    "\n",
    "The hyperparamter $C$ amounts for the budget for the amount by which the margin can be violated by $n$ observations. If $C=0$, there is no budget for the violation of the margin and hence the support vector classifier turns into maximal margin classifier. For $C>0$, no more than $C$ observations can be on the wrong side of the hyperplane (as $\\epsilon_i > 1$ for the observation to be on the wrong side of the hyperplane). If $C$ increases, the margin will widen and if it decreases, the margin will become narrow. $C$ is chosen via <b>cross-validation</b>. When $C$ is <b>small</b>, the margin narrows and hence the classifier fits more colsely to the data, giving a <b>high variance</b>. For <b>larger</b> $C$, the margin is wider and hence fitting the data less hard and obtaining a classifier whihc is <b>more biased</b> but have <b>low variance</b>.\n",
    "\n",
    "The observations that lie on the correct side of the margin do not affect the hyperplane. The observations that either lie on the margin or that violate the margin affect the hyperplane and hence are known as <b>support vectors</b>. For larger value of $C$, we will get a more number of support vectors and hence the hyperplane will depend on large number of observations, making the model low in variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Support Vector Machines\n",
    "\n",
    "#### 9.3.1 Classification with Non-linear Decision Boundaries\n",
    "\n",
    "Support vector classifiers, which are designed to work in the setting of linear decision boundary, can be extended to handle the case of non-linear decision boundary by enlarging the feature space using polynomial transformation of the predictors. For example, if we have a $p$-dimensional feature space given as: $X_1, X_2, ..., X_p$, we could instead fit a support vector classifier using $2p$ features: $X_1, X_1^2, X_2, X_2^2, ..., X_p, X_p^2$. In the transformed feature space, the decision boundary is still linear, but if we consider the original feature space, the decison boundary will be non-linear. There are many ways to enlarge the feature space, such as using higher order polynomials or using interaction terms.\n",
    "\n",
    "#### 9.3.2 The Support Vector Machine\n",
    "\n",
    "The <b>support vector machine(SVM)</b> is an extension of support vector classifier by enlarging the feature space using <b>kernels</b>. For obtaining the solution of the support vector classifier problem, we only need to have the <b>inner products</b> of the observations. The inner product of the observations $x_i$ and $x_{i^{'}}$ is given as:\n",
    "\n",
    "$$\\langle x_i, x_{i^{'}} \\rangle = \\sum_{j=1}^{p} x_{ij} x_{i^{'}j}$$\n",
    "\n",
    "Linear support vector classifier can be represented as:\n",
    "\n",
    "$$f(x) = \\beta_0 + \\sum_{i=1}^{n} \\alpha_i \\langle x, x_i \\rangle$$\n",
    "\n",
    "where $\\alpha_i$s are a total of $n$ parameters, one per training observation. To estimate the parameters $\\beta_0, \\alpha_1, ..., \\alpha_n$, we need to compute ${n}\\choose{2}$ inner products for each combination of training observation. It turns out that if the training observation is <b>not a support vector</b>, the $\\alpha_i$ is 0. Hence, $\\alpha_i$s are non-zero only for support vectors. Hence, the solution can be rewritten in the form\n",
    "\n",
    "$$f(x) = \\beta_0 + \\sum_{i \\in S} \\alpha_i \\langle x, x_i \\rangle$$\n",
    "\n",
    "where $S$ is the set containing the indices of the support vectors. \n",
    "\n",
    "A <b>kernel</b> is a function that <b>quantifies the similarity of two observations</b>. Inner product can be considered as a kernel and hence can be represented as:\n",
    "\n",
    "$$K(x_i, x_{i^{'}}) = \\sum_{j=1}^{p} x_{ij} x_{i^{'}j}$$\n",
    "\n",
    "The above equation is a <b>linear kernel</b> as the support vector classifiers are linear in features. Linear kernel quantifies the similarity of a pair of observation using <b>Pearson (standard) correlation</b>. A kernel of the form:\n",
    "\n",
    "$$K(x_i, x_{i^{'}}) = \\bigg(1 + \\sum_{j=1}^{p} x_{ij} x_{i^{'}j} \\bigg)^d$$\n",
    "\n",
    "is called as <b>polynomial kernel</b> of degree $d$. Using polynomial kernel in a support vector classifier leads to a much more flexible decision boundary. When a support vector classifier is combined with a non-linear kernal it is known as <b>support vector machine</b>. In this case, the function has the form:\n",
    "\n",
    "$$f(x) = \\beta_0 + \\sum_{i \\in S} \\alpha_i K(x, x_{i})$$\n",
    "\n",
    "Apart from polynomial kernel, another popular choice for non-linear kernel is <b>radial kernel</b>, which is given as:\n",
    "\n",
    "$$K(x_i, x_{i^{'}}) = exp \\bigg( - \\gamma \\sum_{j=1}^{p} (x_{ij} - x_{i^{'}j})^2 \\bigg)$$\n",
    "\n",
    "where $\\gamma$ is a positive constant. The intution behind working of radial kernel is as follows:\n",
    "\n",
    "<b><center>For the training observations which are far from the given test observation $x^*$, the Euclidean distance will be large and hence the overall value of $K(x^*, x_i)$ will be small and hence $x_i$ will play virtually no role in $f(x^*)$. In other words, training observations that are far from $x^*$ will play almost no role in the predicted class label for $x^*$ and hence the radial kernel shows a local behaviour.</center></b>\n",
    "\n",
    "One advantage of using kernel instead of using enlarged feature space is the less computational complexity as we just need to compute ${n} \\choose {2} $ values of $K(x_i, x_{i^{'}})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 SVMs with More than Two Classes\n",
    "\n",
    "SVM can be extended for more than two classes by the methods: <b>one-versus-one</b> and <b>one-versus-all</b>.\n",
    "\n",
    "#### 9.4.1 One-Versus-One Classification\n",
    "\n",
    "A <b>one-versus-one</b> or <b>all-pairs</b> classification process builds ${K} \\choose {2}$ SVMs, each of which compares a pair of classes. A test observation is classified using each of the ${K} \\choose {2}$ classifiers. The final classification is done by assigning the test observation to the class it was most frequently assigned by all of these ${K} \\choose {2}$ classifiers.\n",
    "\n",
    "#### 9.4.2 One-Versus-All Classification\n",
    "\n",
    "In <b>one-versus-all</b> approach, $K$ SVMs are fit, each time comparing one of the $K$ classes to remaining $K-1$ classes. The test observation is assigned to the class for which the value of $f(x^*)$ is the largest. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
