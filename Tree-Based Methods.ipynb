{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.text_cell_render { /* Customize text cells */\n",
       "font-family: 'Times New Roman';\n",
       "font-size:1.3em;\n",
       "line-height:1.4em;\n",
       "padding-left:1.5em;\n",
       "padding-right:1.5em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "div.text_cell_render { /* Customize text cells */\n",
    "font-family: 'Times New Roman';\n",
    "font-size:1.3em;\n",
    "line-height:1.4em;\n",
    "padding-left:1.5em;\n",
    "padding-right:1.5em;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Tree-Based Methods</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Tree-based</b> methods <b>stratify</b> or <b>segment</b> the predictor space into a number of simple regions. To make a prediction, we use the <b>mean</b> or the <b>mode</b> of the training observations in the region in which the observation to be predicted belongs. The set of splitting rules can be summarized via a tree, these methods are also known as <b>decision tree</b> methods. <b>Bagging, random forests</b> and <b>boosting</b> produce multiple trees and then combine them in a single model to make the prediction. They provide improved accuracy at the cost of interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 The Basics of Decision Trees\n",
    "#### 8.1.1 Regression Trees\n",
    "##### Predicting Baseball Playersâ€™ Salaries Using Regression Trees\n",
    "\n",
    "The regression tree which predicts the salary of baseball players is shown below. Regression tree consists of a series of splitting rules and stratifies or segments the players into three regions :\n",
    "\n",
    "$$R_1 = \\{X \\ | Years < 4.5\\}$$\n",
    "\n",
    "$$R_2 = \\{X \\ | Years \\geq 4.5, Hits < 117.5\\}$$\n",
    "\n",
    "$$R_3 = \\{X \\ | Years \\geq 4.5, Hits \\geq 117.5\\}$$\n",
    "\n",
    "<img src=\"images/decision_tree.PNG\"  width=\"500px\">\n",
    "\n",
    "The regions $R_1, R_2, R_3$ are known as <b>terminal nodes</b> or <b>leaves</b>. The points along the tree where the predictor space is split is called as <b>internal nodes</b>. The segments of the tree which connect the nodes are called <b>branches</b>. Regression trees are easier to interpret.\n",
    "\n",
    "##### Prediction via Stratification of the Feature Space\n",
    "\n",
    "The regression tree can be build by:\n",
    "\n",
    " -  Dividing the predictor space $X_1, X_2, ..., X_p$ into $J$ <b>distinct</b> and <b>non-overlapping</b> regions $R_1, R_2, ..., R_J$.\n",
    " \n",
    " - Every observation that falls into region $R_j$, the prediction is simply the mean of the training observations in the region.\n",
    " \n",
    "Theoretically, the regions can have any shape. In practice, the regions are divided into high dimensional rectangles for simplicity and ease of interpretation. The goal is to find the regions $R_1, R_2, ..., R_J$ that minimizes\n",
    "\n",
    "$$\\sum_{j=1}^{J}\\sum_{i \\in R_j} (y_i - \\widehat{y_{R_j}})^2$$\n",
    "\n",
    "where $\\widehat{y_{R_j}}$ is the <b>mean</b> of the training observation in the $j$th box. Predictors are split into regions by <b>top-down greedy approach</b> which is known as <b>recursive binary splitting</b>.\n",
    "\n",
    "In recursive binary splitting, we first select a predictor $X_j$ and a <b>cutpoint</b> $s$, such that splitting the predictor space into region $\\{X|X_j < s\\}$ and $\\{X|X_j \\geq s\\}$ leads to the greatest possible reduction in RSS. We consider all possible predictors $X_1, X_2, ..., X_p$, and all possible cutpoints $s$ for each of the predictors and then choose the predictor and cutpoint that leads to lowest possible RSS. To be more specific, for any $j$ and $s$, the pair of half-planes are defined as:\n",
    "\n",
    "$$R_1(j, s) = \\{X|X_j < s\\}$$\n",
    "\n",
    "$$R_2(j, s) = \\{X|X_j \\geq s\\}$$\n",
    "\n",
    "and hence, we need to find the value of $j$ and $s$ which minimizes\n",
    "\n",
    "$$\\sum_{i: x_i \\in R_1(j, s)} (y_i - \\widehat{y_{R_1}})^2 + \\sum_{i: x_i \\in R_2(j, s)} (y_i - \\widehat{y_{R_2}})^2$$\n",
    "\n",
    "This process continues until a stopping criteria is reached (such as no regions contain more than 5 observations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
