{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.text_cell_render { /* Customize text cells */\n",
       "font-family: 'Times New Roman';\n",
       "font-size:1.3em;\n",
       "line-height:1.4em;\n",
       "padding-left:1.5em;\n",
       "padding-right:1.5em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "div.text_cell_render { /* Customize text cells */\n",
    "font-family: 'Times New Roman';\n",
    "font-size:1.3em;\n",
    "line-height:1.4em;\n",
    "padding-left:1.5em;\n",
    "padding-right:1.5em;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Unsupervised Learning</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 The Challenge of Unsupervised Learning\n",
    "\n",
    "<b>Unsupervised learning</b> is often performed as a a part of an <b>exploratory data analysis</b>. In unsupervised learning there is no practical way to assess the performance of the model as we do not have the true answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Principal Components Analysis\n",
    "\n",
    "In the case of a large set of correlated variables, principal component analysis helps in summarizing the data set with a small number of representative variables that explains most of the variability in the data. The principal component directions are the directions in the feature space along which the original data is <b>highly variable</b>. <b>Principal component analysis (PCA)</b> refers to the process by which principal components are computed.\n",
    "\n",
    "#### 10.2.1 What Are Principal Components?\n",
    "\n",
    "To visualize a data set with $n$ observations with a $p$ sets of features $X_1, X_2, ..., X_p$, we can examine $p \\choose 2$ two-dimensional scatter plots of the data. For a large value of $p$, the process is cumbersome and most likely none of them will informative as each contain only a small fraction of total information present in the data. Hence, we would be keen to find a low-dimensional representation of the data that captures as much of the information as possible in the data set. PCA finds this low-dimensional representation of the data set that contains as much as possible of the variation. Each of the dimensions found by PCA is a <b>linear combination</b> of the $p$ features.\n",
    "\n",
    "The <b>first principal component</b> of a set of features $X_1, X_2, ..., X_p$ is the <b>normalized</b> linear combination of the featues\n",
    "\n",
    "$$Z_1 = \\phi_{11}X_1 + \\phi_{21}X_2 + ... + \\phi_{p1}X_p$$\n",
    "\n",
    "that has the <b>largest variance</b>. By <b>normalized</b>, we mean that $\\sum_{j=1}^{p} \\phi_{j1}^2 = 1$. The elements $\\phi_{11}, \\phi_{21}, ... \\phi_{p1}$ are referred to as the <b>loading</b> of the first principal component. Without normalization of the loadings, the  values can be set to arbitrarily large values and hence resulting is a large variance.\n",
    "\n",
    "As for the computation of first principal componenet, we are mainly interested in the variance, we can assume that each of the features in the data set has been centered to have 0 mean. Then, we need to maximize the variance with a constraint that the loadings are normalized. Hence, the first principal component loading vector solves the optimization problem\n",
    "\n",
    "$$maximize_{\\phi_{11}, \\phi_{21}, ..., \\phi_{p1}} \\bigg\\{ \\frac{1}{n} \\sum_{i=1}^{n} \\bigg( \\sum_{j=1}^{p}  \\phi_{j1}x_{ij}\\bigg)^2\\bigg\\} \\ subject \\ \\ to \\ \\ \\sum_{j=1}^{p}\\phi_{j1}^2 = 1$$ \n",
    "\n",
    "The above mentioned objective can instead be written as $\\frac{1}{n} \\sum_{i=1}^{n} z_{i1}^2$. As the mean of $z_i$s will be 0 as well, the objective that we are maximizing is simply the sample varince of the $n$ values of z_{i1}. $z_{11}, z_{21}, ..., z_{n1}$ are called as the <b>scores</b> of the first principal component. This problem can be solved via an <b>eigen decomposition</b>.\n",
    "\n",
    "Geometrically, if we project the $n$ data points $x_1, x_2, ..., x_n$ along the direction of the loading vector of the first principal component, the projected values are the principal componenet scores $z_{11}, z_{21}, ..., z_{n1}$.\n",
    "\n",
    "Similarly, the <b>second principal component</b> is the linear combination of $X_1, X_2, ..., X_p$ that has maximal variance out of all linear combinations that are <b>uncorrelated</b> with $Z_1$. It takes the form \n",
    "\n",
    "$$z_{i2} = \\phi_{12}x_{i1} + \\phi_{22}x_{i2} + ... + \\phi_{p2}x_{ip}$$\n",
    "\n",
    "where $\\phi_2$ is the second principal component loading vector, with elements $\\phi_{12}, \\phi_{22}, ..., \\phi_{p2}$. Constraining $Z_2$ to be uncorrelated with $Z_1$ results in the direction of $\\phi_2$ to be <b>orthogonal</b> of $\\phi_1$. The optimization problem for finding the second principal componenet can be formed in a similar way as the one for the first principal component with an additional constraint that the $\\phi_2$ is orthogonal to $\\phi_1$. It turns out that the principal component directions $\\phi_1, \\phi_2, \\phi_3, ...$ are the ordered sequence of eigenvectors of the matrix $X^TX$, and the eigenvalues are the variance of the components.\n",
    "\n",
    "Usually, PCA is performed after <b>standardizing each variable to have mean 0 and standard deviation 1</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.2.2 Another Interpretation of Principal Components\n",
    "\n",
    "Alternatively, principal components can be viewed as the low-dimensional linear surfaces that are <b>closest</b> to the observations. The first principal component loading vector can be viewed as the line in the $p$-dimensional space that is closest to the $n$ observations (closeness is defined in terms of Euclidean distance). This can be explained simply as we seek a single dimension of the data which is as close as possible to all the data points. It is more likely that this line will explain the variation in data more aptly.\n",
    "\n",
    "This notion can be extended for second and higher principal components as well. The first two principal components can be viewed as the plane that is closest to the $n$ observations in terms of Euclidean distance. Using this interpretation, first $M$ principal components, with their scores and loading vectors, provides the best $M$-dimensional approximation of the data set in terms of the Euclidean distance. For a sufficiently large value of $M$, the $M$ principal components score and loading vectors provide a good approximation for the observations. When $M = min(n-1, p)$, the the representation is exact and hence $x_{ij} = \\sum_{m=1}^{M} z_{im} \\phi_{jm}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.2.3 More on PCA\n",
    "\n",
    "##### Scaling the Variables\n",
    "\n",
    "The results of PCA also depend on the fact that whether <b>the variables are individually scaled or not</b>. If we perform PCA on the unscaled variables, the variables with higher variance will have very <b>large loading</b>. As it is undesirable for the principal components obtained to depend on the scale of the variables, we scale each variables to have the <b>standard deviation 1</b> before performing PCA. If the individual variables are measured in the same unit, the scaling need not to be done.\n",
    "\n",
    "##### Uniqueness of the Principal Components\n",
    "\n",
    "Each principal component loading vecotor is unique upto a <b>sign flip</b>. The sign of the loading vectors may differ as they specify directions in the $p$-dimensional space and hence flipping the sign has no effect. Similarly, the score vectors are unique up to a sign filp as the varince of $Z$ and $-Z$ are same.\n",
    "\n",
    "##### The Proportion of Variance Explained\n",
    "\n",
    "We may be iterested in the amount of variance that has been explained by projecting the data on to first $M$ principal components. This means that we are interested in knowing the <b>proportion of variance explained (PVE)</b> by each principal component. The <b>total variance</b> present in the data set can be given as:\n",
    "\n",
    "$$\\sum_{j=1}^{p} Var(X_j) = \\sum_{j=1}^{p} \\frac{1}{n} \\sum_{i=1}^{n} x_{ij}^2$$\n",
    "\n",
    "and the variance that is explained by the $m^{th}$ principal comonent is:\n",
    "\n",
    "$$\\frac{1}{n} \\sum_{i=1}^{n} z_{im}^2 = \\frac{1}{n} \\sum_{i=1}^{n} \\bigg( \\sum_{j=1}^{p} \\phi_{jm}x_{ij}\\bigg)^2$$\n",
    "\n",
    "Hence, <b>PVE</b> of the first principal component can be given as:\n",
    "\n",
    "$$\\frac{\\sum_{i=1}^{n} \\bigg( \\sum_{j=1}^{p} \\phi_{jm}x_{ij}\\bigg)^2}{\\sum_{j=1}^{p} \\sum_{i=1}^{n} x_{ij}^2}$$\n",
    "\n",
    "In order to obtain the PVE of first $M$ principal components, we can calculate each individual PVEs using the above equation and then sum them all.\n",
    "\n",
    "##### Deciding How Many Principal Components to Use\n",
    "\n",
    "The main goal of PCA is to find the smallest number of principal components that explains a good bit of variance in data. We can decide on the number of principal components to be used by examining the <b>scree plot</b>. It can be done by simply eyeballing the scree plot and finding a point at which the amount of variance explained by subsequent principal component drops off. This point is referred as the <b>elbow</b> in the scree plot.\n",
    "\n",
    "This visual analysis is kind of <b>ad hoc</b> and there is no well-accepted objective way to decide on the number of principal components to be used. The number of principal components to be used depends on the area of application and the nature of the data set.\n",
    "\n",
    "A simple approach is to find the first few principal components and then examine that whether there exists some interesting pattern in the data or not. If no pattern is found in the first few principal components, the subsequent principal components will be of lesser interest as well. This is a subjective approach and reflects on the fact that principal components are used for exploratory data analysis.\n",
    "\n",
    "Principal components can be used for supervised analysis (as in principal component regression) as well. In this case, there is a simple and objective way to decide the number of principal components. The number of principal components to be used can be treated as a tuning parameter and can be decided by cross-validation or similar techniques.\n",
    "\n",
    "#### 10.2.4 Other Uses for Principal Components\n",
    "\n",
    "Like principal component regression, the principal component score vectors can be used as the features in several supervised learning techniques. This will lead to <b>less noisy</b> results as it is often the case that signal in the data set is concentrated in the first few principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Clustering Methods\n",
    "\n",
    "<b>Clustering</b> is a technique for finding <b>subgroups</b> or <b>clusters</b> in a data set based on similarity between individual observations. For clustering, we need to define the measure of similarity which depends on the knowledge of the data set. Two best known clustering methods are <b>K-means clustering</b> and <b>hierarchical clustering</b>. In K-means clustering, we partition the observations into a pre-defined number of clusters. In hierarchical clustering, the number of clusters is unknown and the results of clustering is represented as a <b>dendrogram</b>, which is a tree-like visualization technique that allows us to view the clustering results for various number of clusters (from 1 to $n$).\n",
    "\n",
    "#### 10.3.1 K-Means Clustering\n",
    "\n",
    "To perform K-means clustering, we first specify the number of clusters $K$ and then the K-means algorithm is used to assign each observation to exactly one of the $K$ clusters. Let $C_1, C_2, ..., C_K$ denote sets containing the indices of the observations in each cluster, then these sets satisfy two properties:\n",
    "\n",
    " - $C_1 \\bigcup C_2 \\bigcup ... \\bigcup C_K = \\{1,2,3,...,n\\}$.\n",
    " \n",
    " \n",
    " - $C_k \\bigcap C_{k^{'}} = \\Phi$ for all $k \\neq k^{'}$.\n",
    " \n",
    "In a good clustering, the <b>within-cluster variation</b> is as small as possible. The within-cluster variation, denoted as $W(C_k)$, is a measure of the amount by which the observations within a cluster differ from each other. Hence, for clustering, we want to solve the problem\n",
    "\n",
    "$$minimize_{C_1, C_2, ..., C_K} \\bigg \\{ \\sum_{k=1}^{K} W(C_k)\\bigg \\}$$\n",
    "\n",
    "In order to solve the above problem, we need to define the within-clustre variation more concretely. The most common choice is <b>squared Euclidean distance</b>, which is defined as\n",
    "\n",
    "$$W(C_k) = \\frac{1}{|C_k|} \\sum_{i, i^{'} \\in C_k} \\sum_{j=1}^{p} (x_{ij} - x_{i^{,}j})^2$$\n",
    "\n",
    "where $|C_k|$ denotes the number of observations in the $k^{th}$ cluster. Hence, the K-means clustering problem can be defined as:\n",
    "\n",
    "$$minimize_{C_1, C_2, ..., C_K} \\bigg \\{ \\sum_{k=1}^{K} \\frac{1}{|C_k|} \\sum_{i, i^{'} \\in C_k} \\sum_{j=1}^{p} (x_{ij} - x_{i^{,}j})^2 \\bigg \\}$$\n",
    "\n",
    "The above mentioned problem is a very difficlut problem to solve as there are $K^n$ ways to divide $n$ observations into $K$ clusters. Instead, an algorithm that gives a local optimal soultion exists and is given as:\n",
    "\n",
    " - Randomly assign a number from 1 to $K$ to all the individual observations. This serves as the initial cluster assignments for the observations\n",
    " \n",
    " \n",
    " - Iterate the cluster assignments (by calculating the cluster <b>centroids</b> and reassigning the observations to the clusters to which it is the nearest) until the cluster assignments stop changing.\n",
    " \n",
    "As the K-means clustering algorithm finds a local-optimal solution, the result will depend on the initial random cluster assignments. Hence, it is important to run the algorithm multiple times with different initial random seeds and the select the <b>best solution</b> (for which the objective is minimum).\n",
    "\n",
    "#### 10.3.2 Hierarchical Clustering\n",
    "\n",
    "K-means clustering has a disadvantage that there is a need to pre-specify the number of clusters $K$. Hierarchical clutsring is an alternative approach which is free from this problem which results in an altarnative tree-based representation of the observations, called as <b>dendrogram</b>.\n",
    "\n",
    "The most common technique used for hierarchical clustering is <b>bottom-up</b> or <b>agglomerative</b> clustering. It is based on the fact that the dendrogram (generally depicted as an upside-down tree) is built starting from leaves and combining the clusters up to the trunk.\n",
    "\n",
    "##### Interpreting a Dendrogram\n",
    "\n",
    "Each leaf of a dendrogram represents an observation. As we move up the tree, some leaves begin to <b>fuse</b> into branches. This correspoonds to observations that are similar to each other. As we further move up, more fusion occurs (either of branches or of a branch and a leaf). <b>Earlier (lower) the fusion occurs, more similar the group of observations to each other</b>. Obervations or groups that fuse later (near the top of the tree) can be quite different from each other. Or, <b>height of the fusion on vertical axis</b> represents how different the two observations are.\n",
    "    \n",
    "One common misunderstanding while interpreting the dendrogram is deriving conclusions based on the distance along <b>horizontal axis</b>. It should be noted that distace between two observations on horizontal axis purely depends on the initial reprentation of observations. This does not measure any similarity between the observations. The similarity between the observations is only measured by the <b>location of the fusion on the vertical axis</b>.\n",
    "\n",
    "To obtain the clustres on the basis of a dendrogram, we make a <b>horizontal cut</b> across the dendrogram. The lower the cut is made, the more number of clusters obtained. Hence, the height of the cut to the dendrogram serves the same role as the $K$ in $K$-means clustering. It controls the number of clustres obtained.\n",
    "\n",
    "##### The Hierarchical Clustering Algorithm\n",
    "\n",
    "To obtain hierarchical clustering, first of all, we define some sort of <b>dissimilarity measure</b>. The most common choice is the Euclidean distance. Each of the individual $n$ observations is then treated as its own cluster. The two clusters that are the most similar (based on the dissimilarity measure) to each other are then <b>fused</b> to obtain $n-1$ clusters. The process is repeated until all the observations belong to one single cluster, and the dendrogram is complete.\n",
    "\n",
    "To fuse individual clusters, the notion of dissimilarity between pair of observations needs to be extended to a pair of <b>groups of observations</b>. This extension is achieved by developing the notion of <b>linkage</b> which defines the dissimilarity between two groups of observations. The most common types of linkage are: <b>complete, average, single</b> and <b>centroid</b>. For the computation of complete, single and average linkage, we find the <b>interclustre dissimilarity</b>, the pairwise dissimilarities between the observations in cluster $A$ and $B$, and record the <b>largest, smallest</b> and <b>mean</b> value as the measure respectively. In the centroid linkage, the dissimilarity between the centroid of clutser $A$ and $B$ serves the purpose.\n",
    "\n",
    "Average, complete and single linkage are most popular among statiscians. Average and complete linkage generally give a more balanced dendrogram and hence is preffered over single linkage. Centroid linkage suffers from the problem of <b>inversion</b>, whereby two clusters are fused at a height <b>below</b> either of the individual clutsers in the dendrogram.  \n",
    "\n",
    "##### Choice of Dissimilarity Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
